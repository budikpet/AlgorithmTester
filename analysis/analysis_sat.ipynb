{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of output files\n",
    "## Prepare environment, functions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas\n",
    "import matplotlib\n",
    "import numpy\n",
    "from algorithm_tester.helpers import FilePair\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Zapnout zobrazování grafů (procento uvozuje „magickou” zkratku IPythonu):\n",
    "%matplotlib inline\n",
    "\n",
    "path = 'tester_results'\n",
    "evo_path = f'{path}/EvoFile'\n",
    "sol_path = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas.set_option('display.max_rows', None)\n",
    "#pandas.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths_from_dir(path: str, include_sol: bool = False, include_instance: bool = True) -> (str, str, str):\n",
    "    dataset_prefix: str = path.split(\"/\")[-1]\n",
    "    for root, _, files in os.walk(path):\n",
    "        dataset: str = dataset_prefix + \"_\" + \"_\".join(root.replace(path, \"\")[1:].split('/'))\n",
    "        for file in files:\n",
    "            if \"column\" not in file and \".dat\" in file:\n",
    "                if (\"_sol\" in file and include_sol) or (\"_inst\" in file and include_instance):\n",
    "                    yield (dataset, root, file)\n",
    "\n",
    "def get_cols_list(path: str):\n",
    "    cols = pandas.read_csv(path, index_col=None, delimiter=\" \", header=None)\n",
    "    return list(cols.iloc[0])\n",
    "\n",
    "def load_sol_from_dir(folder_path: str, column_list, special_column_name: str = \"instance_info\"):\n",
    "    \"\"\" Load solutions from directory files into table. \"\"\"\n",
    "    it = get_file_paths_from_dir(folder_path, include_sol=True, include_instance=False)\n",
    "    output_table = None\n",
    "        \n",
    "    for (dataset, root, file) in it:\n",
    "        curr_table = pandas.read_csv(f'{root}/{file}', index_col=None, delimiter=\" \", header=None).iloc[:,0:3]\n",
    "        curr_table.columns = column_list\n",
    "        curr_table[\"dataset\"] = dataset\n",
    "        curr_table[special_column_name] = file.split(\"_\")[1]\n",
    "                \n",
    "        if output_table is not None:\n",
    "            output_table = output_table.append(curr_table, ignore_index=True)\n",
    "        else:\n",
    "            output_table = curr_table\n",
    "    \n",
    "    return output_table\n",
    "\n",
    "def load_data_from_dir(folder_path: str, column_list, special_column_name: str = \"instance_info\"):\n",
    "    \"\"\" Load data from directory files into table. \"\"\"\n",
    "    it = get_file_paths_from_dir(folder_path)\n",
    "    output_table = None\n",
    "    \n",
    "    for (dataset, root, file) in it:\n",
    "        curr_table = pandas.read_csv(f'{root}/{file}', index_col=None, delimiter=\" \", header=None)\n",
    "        curr_table.columns = column_list\n",
    "        curr_table[\"dataset\"] = dataset\n",
    "        curr_table[special_column_name] = file.split(\"_\")[1]\n",
    "                \n",
    "        if output_table is not None:\n",
    "            output_table = output_table.append(curr_table, ignore_index=True)\n",
    "        else:\n",
    "            output_table = curr_table\n",
    "    \n",
    "    #output_table = output_table.set_index(['algorithm', 'dataset', 'id', \"item_count\"])\n",
    "    #output_table.sort_values(by=[\"algorithm\", \"dataset\", \"item_count\", \"id\"], inplace=True)\n",
    "    return output_table\n",
    "\n",
    "def construct_table_from(filePair: FilePair):\n",
    "    solution_table = pandas.read_csv(filePair.solutionFile, header=None, index_col=None, delimiter=\" \")\n",
    "    data_table = pandas.read_csv(filePair.dataFile, header=None, index_col=None, delimiter=\" \")\n",
    "    \n",
    "    item_count = data_table.iloc[0, 1]\n",
    "    \n",
    "    solution_table = solution_table.drop_duplicates(subset=[0], keep='first').reset_index()\n",
    "\n",
    "    data_table = data_table.iloc[:, 4:]\n",
    "    data_table = data_table[data_table.columns[::2]]\n",
    "\n",
    "    info_table = pandas.concat([solution_table.iloc[:, 1], solution_table.iloc[:, 3], data_table.max(axis=1)], axis=1)\n",
    "    info_table.columns = [\"id\", \"best_value\", \"max_cost\"]\n",
    "    info_table[\"item_count\"] = item_count\n",
    "    return info_table\n",
    "\n",
    "def create_avg_time_table(table, name: str, column_name: str = \"item_count\"):\n",
    "    # Create a table of average times according to algorithm and item_count columns\n",
    "    avg_times = table.groupby([\"algorithm_name\", column_name])['elapsed_time'] \\\n",
    "        .mean().reset_index()\n",
    "    avg_times = avg_times.round(2)\n",
    "    \n",
    "    avg_configs = table.groupby([\"algorithm_name\", column_name])['elapsed_configs']\\\n",
    "        .mean().reset_index()\n",
    "    \n",
    "    avg_times = avg_times.merge(avg_configs, on=[\"algorithm_name\", column_name])\n",
    "\n",
    "    # Move all values of algorithm column into separate columns\n",
    "    #avg_times = avg_times.unstack(\"algorithm_name\")\n",
    "    #avg_times.columns = avg_times.columns.droplevel()\n",
    "    avg_times.name = f\"Avg #configs per {column_name}\"\n",
    "    avg_times.sort_values(by=column_name, inplace=True)\n",
    "    #avg_times.fillna(\"-\", inplace=True)\n",
    "\n",
    "    # Save the dataframe to csv\n",
    "    #avg_times.to_excel(f'excel/{name}_avg_times.xlsx', sheet_name=name)\n",
    "    \n",
    "    return avg_times\n",
    "\n",
    "def create_avg_error_table(table, column_name: str, table_name: str = \"unknown\"):\n",
    "    table[\"relative_error\"] = numpy.abs(table[\"best_value\"] - table[\"found_value\"])/table[\"best_value\"]\n",
    "    table = table.fillna(0)\n",
    "    \n",
    "    error_group = table.groupby([column_name, \"algorithm_name\"])[\"relative_error\"]\n",
    "\n",
    "    error_max = error_group.max().reset_index() \\\n",
    "        .rename(columns={'relative_error':'max_relative_error'})\n",
    "    error_avg = error_group.mean().reset_index() \\\n",
    "        .rename(columns={'relative_error':'avg_relative_error'})\n",
    "    \n",
    "    # Construct, unstack\n",
    "    avg_error = pandas.merge(error_max, error_avg, on=[column_name, \"algorithm_name\"])\n",
    "    #avg_error = avg_error.set_index([\"algorithm_name\", column_name]).unstack(\"algorithm_name\")\n",
    "    #avg_error = error_max.join(error_avg).round(6)\n",
    "    #avg_error.columns = [\"max_relative_error\", \"avg_relative_error\"]\n",
    "    avg_error.name = f\"Avg & max relative error per {column_name}\"\n",
    "    \n",
    "    \n",
    "    avg_error.set_index([\"algorithm_name\", column_name]).unstack(\"algorithm_name\")\\\n",
    "        .round(6)\\\n",
    "        .to_excel(f\"excel/{table_name}_avg_error.xlsx\", sheet_name=table_name)\n",
    "    \n",
    "    return avg_error\n",
    "\n",
    "def save_plot(table, title: str, column_name: str, output_name: str, y_label: str = \"Relative errors\"):\n",
    "    worktable = table.loc[:, column_name].copy()\n",
    "    \n",
    "    plot = worktable.plot()\n",
    "    plot.set_ylabel(y_label)\n",
    "\n",
    "    figure = plot.get_figure()\n",
    "    figure.suptitle(title)\n",
    "    figure.savefig(f\"excel/{output_name}.pdf\")\n",
    "    \n",
    "    return plot\n",
    "\n",
    "def save_table(table, output_name):\n",
    "    table.round(6).to_excel(f\"excel/{output_name}_table.xlsx\", sheet_name=output_name)\n",
    "    \n",
    "def init_evo_plot(title: str):\n",
    "    fig, axes = plt.subplots(1, 1, sharex=True, sharey=True, figsize=[13,5])\n",
    "    axes.set_xlabel('step')\n",
    "    axes.set_ylabel('Sum of costs')\n",
    "    axes.set_title(title)\n",
    "    return fig, axes\n",
    "    \n",
    "def add_evo_plot(path: str, columns, plot_name: str, axes):\n",
    "    evo_table = pandas.read_csv(path, index_col=None, delimiter=\" \", header=None)\n",
    "    evo_table.columns = evo_cols\n",
    "    evo_table[\"step\"] = range(evo_table['current_temperature'].count())\n",
    "    \n",
    "    data = evo_table.sort_values(by=\"step\").set_index([\"step\"]).loc[:, \"best_cost\"]\n",
    "    \n",
    "    axes.plot(data, label=f\"best_cost_{plot_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column lists\n",
    "\n",
    "sol_cols = [\"output_filename\", \"best_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output_filename</th>\n",
       "      <th>best_value</th>\n",
       "      <th>found_value</th>\n",
       "      <th>num_of_vars</th>\n",
       "      <th>num_of_clauses</th>\n",
       "      <th>algorithm_name</th>\n",
       "      <th>init_temperature</th>\n",
       "      <th>min_temperature</th>\n",
       "      <th>cooling</th>\n",
       "      <th>cycles</th>\n",
       "      <th>elapsed_configs</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>relative_mistake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uf20-01000</td>\n",
       "      <td>73</td>\n",
       "      <td>56</td>\n",
       "      <td>20</td>\n",
       "      <td>88</td>\n",
       "      <td>SA_SAT_V1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>55</td>\n",
       "      <td>835945</td>\n",
       "      <td>30097.468862</td>\n",
       "      <td>0.767123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uf20-0102</td>\n",
       "      <td>67</td>\n",
       "      <td>58</td>\n",
       "      <td>20</td>\n",
       "      <td>88</td>\n",
       "      <td>SA_SAT_V1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>55</td>\n",
       "      <td>835945</td>\n",
       "      <td>29829.199810</td>\n",
       "      <td>0.865672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uf20-0107</td>\n",
       "      <td>80</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>88</td>\n",
       "      <td>SA_SAT_V1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>55</td>\n",
       "      <td>835945</td>\n",
       "      <td>29567.651842</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uf20-0108</td>\n",
       "      <td>75</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>88</td>\n",
       "      <td>SA_SAT_V1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>55</td>\n",
       "      <td>835945</td>\n",
       "      <td>29095.706479</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uf20-0110</td>\n",
       "      <td>69</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>88</td>\n",
       "      <td>SA_SAT_V1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>55</td>\n",
       "      <td>835945</td>\n",
       "      <td>30275.437377</td>\n",
       "      <td>0.536232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>uf50-0897</td>\n",
       "      <td>96646</td>\n",
       "      <td>56298</td>\n",
       "      <td>50</td>\n",
       "      <td>201</td>\n",
       "      <td>SA_SAT_V1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>55</td>\n",
       "      <td>835945</td>\n",
       "      <td>41549.245319</td>\n",
       "      <td>0.582518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>uf50-0920</td>\n",
       "      <td>77220</td>\n",
       "      <td>79620</td>\n",
       "      <td>50</td>\n",
       "      <td>201</td>\n",
       "      <td>SA_SAT_V1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>55</td>\n",
       "      <td>835945</td>\n",
       "      <td>36056.068310</td>\n",
       "      <td>1.031080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>uf50-0954</td>\n",
       "      <td>82382</td>\n",
       "      <td>66163</td>\n",
       "      <td>50</td>\n",
       "      <td>201</td>\n",
       "      <td>SA_SAT_V1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>55</td>\n",
       "      <td>835945</td>\n",
       "      <td>35481.305337</td>\n",
       "      <td>0.803124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>uf50-0957</td>\n",
       "      <td>67103</td>\n",
       "      <td>64070</td>\n",
       "      <td>50</td>\n",
       "      <td>201</td>\n",
       "      <td>SA_SAT_V1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>55</td>\n",
       "      <td>835945</td>\n",
       "      <td>36127.896180</td>\n",
       "      <td>0.954801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>uf50-0987</td>\n",
       "      <td>71746</td>\n",
       "      <td>96764</td>\n",
       "      <td>50</td>\n",
       "      <td>201</td>\n",
       "      <td>SA_SAT_V1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>55</td>\n",
       "      <td>835945</td>\n",
       "      <td>36331.873889</td>\n",
       "      <td>1.348702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4701 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   output_filename  best_value  found_value  num_of_vars  num_of_clauses  \\\n",
       "0       uf20-01000          73           56           20              88   \n",
       "1        uf20-0102          67           58           20              88   \n",
       "2        uf20-0107          80           32           20              88   \n",
       "3        uf20-0108          75           21           20              88   \n",
       "4        uf20-0110          69           37           20              88   \n",
       "..             ...         ...          ...          ...             ...   \n",
       "29       uf50-0897       96646        56298           50             201   \n",
       "30       uf50-0920       77220        79620           50             201   \n",
       "31       uf50-0954       82382        66163           50             201   \n",
       "32       uf50-0957       67103        64070           50             201   \n",
       "33       uf50-0987       71746        96764           50             201   \n",
       "\n",
       "   algorithm_name  init_temperature  min_temperature  cooling  cycles  \\\n",
       "0       SA_SAT_V1            1000.0              0.5   0.9995      55   \n",
       "1       SA_SAT_V1            1000.0              0.5   0.9995      55   \n",
       "2       SA_SAT_V1            1000.0              0.5   0.9995      55   \n",
       "3       SA_SAT_V1            1000.0              0.5   0.9995      55   \n",
       "4       SA_SAT_V1            1000.0              0.5   0.9995      55   \n",
       "..            ...               ...              ...      ...     ...   \n",
       "29      SA_SAT_V1            1000.0              0.5   0.9995      55   \n",
       "30      SA_SAT_V1            1000.0              0.5   0.9995      55   \n",
       "31      SA_SAT_V1            1000.0              0.5   0.9995      55   \n",
       "32      SA_SAT_V1            1000.0              0.5   0.9995      55   \n",
       "33      SA_SAT_V1            1000.0              0.5   0.9995      55   \n",
       "\n",
       "    elapsed_configs  elapsed_time  relative_mistake  \n",
       "0            835945  30097.468862          0.767123  \n",
       "1            835945  29829.199810          0.865672  \n",
       "2            835945  29567.651842          0.400000  \n",
       "3            835945  29095.706479          0.280000  \n",
       "4            835945  30275.437377          0.536232  \n",
       "..              ...           ...               ...  \n",
       "29           835945  41549.245319          0.582518  \n",
       "30           835945  36056.068310          1.031080  \n",
       "31           835945  35481.305337          0.803124  \n",
       "32           835945  36127.896180          0.954801  \n",
       "33           835945  36331.873889          1.348702  \n",
       "\n",
       "[4701 rows x 13 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_full_table_for_dataset(instance_path: str, sol_path: str, instance_cols, sol_cols):\n",
    "    output_table = None\n",
    "    \n",
    "    # Get all solutions from all files\n",
    "    output_table = pandas.read_csv(instance_path, index_col=None, delimiter=\" \", header=None)\\\n",
    "        .iloc[:, [0, 1,-9,-8,-7,-6,-5,-4,-3,-2,-1]]\n",
    "    output_table.columns = instance_cols\n",
    "\n",
    "    # Add data from solution file\n",
    "    sols_table = pandas.read_csv(sol_path, index_col=None, delimiter=\" \", header=None)\\\n",
    "        .iloc[:, [0, 1]]\n",
    "    sols_table.columns = sol_cols\n",
    "    sols_table.drop_duplicates(subset=\"output_filename\", inplace=True)\n",
    "        \n",
    "    output_table = pandas.merge(sols_table, output_table, on=[\"output_filename\"])\n",
    "    output_table = output_table.astype({'found_value': 'int64'})\n",
    "    output_table[\"relative_mistake\"] = output_table[\"found_value\"]/output_table[\"best_value\"]\n",
    "    \n",
    "    return output_table\n",
    "\n",
    "def get_all_tables_of_results(base: str):\n",
    "    cols = get_cols_list(f'{base}/column_description.dat')\n",
    "    cols.remove(\"vars_output\")\n",
    "        \n",
    "    a1 = get_full_table_for_dataset(f'{base}/wuf-A/wuf20-88-A.dat', f'{sol_path}/wuf-A/wuf20-88-A-opt.dat', cols, sol_cols)\n",
    "    a2 = get_full_table_for_dataset(f'{base}/wuf-A/wuf20-91-A.dat', f'{sol_path}/wuf-A/wuf20-91-A-opt.dat', cols, sol_cols)\n",
    "    \n",
    "    m1 = get_full_table_for_dataset(f'{base}/wuf-M/wuf20-78-M.dat', f'{sol_path}/wuf-M/wuf20-78-M-opt.dat', cols, sol_cols)\n",
    "    m2 = get_full_table_for_dataset(f'{base}/wuf-M/wuf50-201-M.dat', f'{sol_path}/wuf-M/wuf50-201-M-opt.dat', cols, sol_cols)\n",
    "    \n",
    "    n1 = get_full_table_for_dataset(f'{base}/wuf-N/wuf20-78-N.dat', f'{sol_path}/wuf-N/wuf20-78-N-opt.dat', cols, sol_cols)\n",
    "    n2 = get_full_table_for_dataset(f'{base}/wuf-N/wuf50-201-N.dat', f'{sol_path}/wuf-N/wuf50-201-N-opt.dat', cols, sol_cols)\n",
    "\n",
    "    q1 = get_full_table_for_dataset(f'{base}/wuf-Q/wuf20-78-Q.dat', f'{sol_path}/wuf-Q/wuf20-78-Q-opt.dat', cols, sol_cols)\n",
    "    q2 = get_full_table_for_dataset(f'{base}/wuf-Q/wuf50-201-Q.dat', f'{sol_path}/wuf-Q/wuf50-201-Q-opt.dat', cols, sol_cols)\n",
    "    \n",
    "    r1 = get_full_table_for_dataset(f'{base}/wuf-R/wuf20-78-R.dat', f'{sol_path}/wuf-R/wuf20-78-R-opt.dat', cols, sol_cols)\n",
    "    r2 = get_full_table_for_dataset(f'{base}/wuf-R/wuf50-201-R.dat', f'{sol_path}/wuf-R/wuf50-201-R-opt.dat', cols, sol_cols)\n",
    "\n",
    "    dfs = [a1, a2, m1, m2, n1, n2, q1, q2, r1, r2]\n",
    "    \n",
    "    return pandas.concat(dfs)\n",
    "\n",
    "get_all_tables_of_results(\"tester_results_V2_moreAccurate\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put data from all analysis files into tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all tables\n",
    "\n",
    "wuf_A_88 = get_full_table_for_dataset(f'{path}/wuf-A/wuf20-88-A', f'{sol_path}/wuf-A/wuf20-88-A-opt.dat', cols, sol_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_nk = create_avg_error_table(nk_table, \"item_count\")\n",
    "error_zkc = create_avg_error_table(zkc_table, \"item_count\")\n",
    "error_zkw = create_avg_error_table(zkw_table, \"item_count\")\n",
    "\n",
    "time_nk = create_avg_time_table(nk_table, name=\"initial temperature\", column_name=\"item_count\")\n",
    "time_zkc = create_avg_time_table(zkc_table, name=\"cooling coefficient\", column_name=\"item_count\")\n",
    "time_zkw = create_avg_time_table(zkw_table, name=\"number of cycles\", column_name=\"item_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table = nk_table.append(zkc_table).append(zkw_table).query(\"algorithm_name != 'SA_OLD'\")\n",
    "sapenalty_error_comp = create_avg_error_table(full_table, \"item_count\", \"sapenalty_comp\")\\\n",
    "    .set_index([\"item_count\", \"algorithm_name\"]).unstack(\"algorithm_name\")\\\n",
    "    .drop(columns=\"max_relative_error\")\n",
    "\n",
    "sapenalty_comp_plot = sapenalty_error_comp.plot.bar(legend=True)\n",
    "sapenalty_comp_plot.set_ylabel(\"Relative errors\")\n",
    "\n",
    "figure = sapenalty_comp_plot.get_figure()\n",
    "figure.suptitle(\"SA/SAPenalty relative error comparison\")\n",
    "figure.savefig(\"excel/sapenalty_comp.pdf\")\n",
    "\n",
    "sapenalty_comp_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter analysis\n",
    "## Analyze the same dataset, change one of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sols = load_sol_from_dir(f'{solutions}/ParamAnalysis', sol_cols) \\\n",
    "    .drop(columns=\"dataset\")\n",
    "\n",
    "init_temp_table = load_data_from_dir(f'{path}/ParamAnalysis/InitTemperature', cols) \\\n",
    "    .drop(columns=[\"things\", \"cycles\", \"min_temperature\", \"cooling\"]) \\\n",
    "    .merge(param_sols, on=[\"id\", \"item_count\"])\n",
    "init_temp_table[\"init_temp\"] = init_temp_table[\"dataset\"]\\\n",
    "    .str.split(\"_\", n = 1, expand = True)[1].astype(int)\n",
    "\n",
    "cooling_table = load_data_from_dir(f'{path}/ParamAnalysis/Cooling', cols) \\\n",
    "    .drop(columns=[\"things\", \"cycles\", \"min_temperature\", \"init_temperature\"]) \\\n",
    "    .merge(param_sols, on=[\"id\", \"item_count\"])\n",
    "cooling_table[\"cooling_coef\"] = cooling_table[\"dataset\"]\\\n",
    "    .str.split(\"_\", n = 1, expand = True)[1].str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "cycles_table = load_data_from_dir(f'{path}/ParamAnalysis/Cycles', cols) \\\n",
    "    .drop(columns=[\"things\", \"init_temperature\", \"min_temperature\", \"cooling\"]) \\\n",
    "    .merge(param_sols, on=[\"id\", \"item_count\"])\n",
    "cycles_table[\"cycles\"] = cycles_table[\"dataset\"]\\\n",
    "    .str.split(\"_\", n = 1, expand = True)[1].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_init_temp = create_avg_error_table(init_temp_table, \"init_temp\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"init_temp\")\n",
    "error_cooling = create_avg_error_table(cooling_table, \"cooling_coef\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"cooling_coef\")\n",
    "error_cycles = create_avg_error_table(cycles_table, \"cycles\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"cycles\")\n",
    "\n",
    "time_init_temp = create_avg_time_table(init_temp_table, name=\"initial temperature\", column_name=\"init_temp\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"init_temp\")\n",
    "time_cooling = create_avg_time_table(cooling_table, name=\"cooling coefficient\", column_name=\"cooling_coef\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"cooling_coef\")\n",
    "time_cycles = create_avg_time_table(cycles_table, name=\"number of cycles\", column_name=\"cycles\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"cycles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_init_temp)\n",
    "\n",
    "out = pandas.merge(error_init_temp, time_init_temp, on=[\"algorithm_name\", \"init_temp\"])\n",
    "save_table(out, \"init_temp_errors_speed\")\n",
    "save_plot(error_init_temp, \"Avg error - init temperatures\", \"avg_relative_error\", \"init_temp_avg_error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_init_temp)\n",
    "\n",
    "save_plot(time_init_temp, \"Avg speed - init temperatures\", \"elapsed_time\", \"init_temp_time_ms\", \"Time[ms]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_init_temp, \"Avg speed - init temperatures\", \"elapsed_configs\", \"init_temp_time_configs\", \"Time[configs]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = init_evo_plot(\"Evolution of costs for different init temperatures\")\n",
    "\n",
    "add_evo_plot(f\"{evo_path}/InitTemperature/EvoFile_40_inst_SA_1500.evo\", evo_cols, \"1500\", axes)\n",
    "add_evo_plot(f\"{evo_path}/InitTemperature/EvoFile_40_inst_SA_2500.evo\", evo_cols, \"2500\", axes)\n",
    "axes.legend()\n",
    "\n",
    "fig.savefig(f\"excel/init_temp_evo.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_cooling)\n",
    "\n",
    "out = pandas.merge(error_cooling, time_cooling, on=[\"algorithm_name\", \"cooling_coef\"])\n",
    "save_table(out, \"cooling_errors_speed\")\n",
    "save_plot(error_cooling, \"Avg error - cooling\", \"avg_relative_error\", \"cooling_avg_error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_cooling, \"Avg speed - cooling\", \"elapsed_time\", \"cooling_time_ms\", \"Time[ms]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_cooling, \"Avg speed - cooling\", \"elapsed_configs\", \"cooling_time_configs\", \"Time[configs]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = init_evo_plot(\"Evolution of costs for different cooling\")\n",
    "\n",
    "add_evo_plot(f\"{evo_path}/Cooling/EvoFile_40_inst_SA_950.evo\", evo_cols, \"950\", axes)\n",
    "add_evo_plot(f\"{evo_path}/Cooling/EvoFile_40_inst_SA_970.evo\", evo_cols, \"970\", axes)\n",
    "\n",
    "axes.legend()\n",
    "\n",
    "fig.savefig(f\"excel/cooling_evo.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_cycles)\n",
    "\n",
    "out = pandas.merge(error_cycles, time_cycles, on=[\"algorithm_name\", \"cycles\"])\n",
    "save_table(out, \"cycles_errors_speed\")\n",
    "save_plot(error_cycles, \"Avg error - cycles\", \"avg_relative_error\", \"cycles_avg_error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_cycles, \"Avg speed - cycles\", \"elapsed_time\", \"cycles_time_configs\", \"Time[ms]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_cycles, \"Avg speed - cycles\", \"elapsed_configs\", \"cycles_time_ms\", \"Time[configs]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = init_evo_plot(\"Evolution of costs for different cycles\")\n",
    "\n",
    "add_evo_plot(f\"{evo_path}/Cycles/EvoFile_40_inst_SA_75.evo\", evo_cols, \"75\", axes)\n",
    "add_evo_plot(f\"{evo_path}/Cycles/EvoFile_40_inst_SA_100.evo\", evo_cols, \"100\", axes)\n",
    "\n",
    "axes.legend()\n",
    "\n",
    "fig.savefig(f\"excel/cycles_evo.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataAnalysis\n",
    "## Analyze datasets that were generated differently with fixed SA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath = f'{path}/DataAnalysis'\n",
    "dsolutions = f'{solutions}/DataAnalysis'\n",
    "\n",
    "balance_table = load_data_from_dir(f'{dpath}/Balance', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/Balance', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'balance'})\n",
    "correlation_table = load_data_from_dir(f'{dpath}/Correlation', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/Correlation', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'correlation'})\n",
    "granularity_heavy_table = load_data_from_dir(f'{dpath}/GranularityHeavy', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/GranularityHeavy', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'constant'})\n",
    "granularity_light_table = load_data_from_dir(f'{dpath}/GranularityLight', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/GranularityLight', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'constant'})\n",
    "maxcost_table = load_data_from_dir(f'{dpath}/MaxCost', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/MaxCost', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'maxcost'})\n",
    "maxweight_table = load_data_from_dir(f'{dpath}/MaxWeight', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/MaxWeight', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'maxweight'})\n",
    "things_table = load_data_from_dir(f'{dpath}/Things', cols)\n",
    "weight_cap_ratio_table = load_data_from_dir(f'{dpath}/WeightCapRation', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/WeightCapRation', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'ratio'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_balance = create_avg_error_table(balance_table, \"balance\")\n",
    "error_corr = create_avg_error_table(correlation_table, \"correlation\")\n",
    "error_granularity_heavy = create_avg_error_table(granularity_heavy_table, \"constant\")\n",
    "error_granularity_light = create_avg_error_table(granularity_light_table, \"constant\")\n",
    "error_maxcost = create_avg_error_table(maxcost_table, \"maxcost\")\n",
    "error_maxweight = create_avg_error_table(maxweight_table, \"maxweight\")\n",
    "error_weightcap = create_avg_error_table(weight_cap_ratio_table, \"ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_avg_error_table(, \"maxcost\")\n",
    "print(error_balance.query('algorithm_name == \"SA\"'))\n",
    "error_balance.query('algorithm_name == \"SA\"').set_index(\"balance\").loc[:, \"avg_relative_error\"].plot.bar(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_corr.query('algorithm_name == \"SA\"'))\n",
    "error_corr.query('algorithm_name == \"SA\"').set_index(\"correlation\").loc[:, \"avg_relative_error\"].plot.bar(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_granularity_heavy.query('algorithm_name == \"SA\"'))\n",
    "error_granularity_heavy.query('algorithm_name == \"SA\"').set_index(\"constant\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_granularity_light.query('algorithm_name == \"SA\"'))\n",
    "error_granularity_light.query('algorithm_name == \"SA\"').set_index(\"constant\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_maxcost.query('algorithm_name == \"SA\"'))\n",
    "error_maxcost.query('algorithm_name == \"SA\"').set_index(\"maxcost\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_maxweight.query('algorithm_name == \"SA\"'))\n",
    "error_maxweight.query('algorithm_name == \"SA\"').set_index(\"maxweight\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_weightcap.query('algorithm_name == \"SA\"'))\n",
    "error_weightcap.query('algorithm_name == \"SA\"').set_index(\"ratio\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
