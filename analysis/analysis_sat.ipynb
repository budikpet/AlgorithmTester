{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of output files\n",
    "## Prepare environment, functions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas\n",
    "import matplotlib\n",
    "import numpy\n",
    "from algorithm_tester.helpers import FilePair\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Zapnout zobrazování grafů (procento uvozuje „magickou” zkratku IPythonu):\n",
    "%matplotlib inline\n",
    "\n",
    "path = 'tester_results'\n",
    "evo_path = f'{path}/EvoFile'\n",
    "sol_path = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas.set_option('display.max_rows', None)\n",
    "#pandas.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths_from_dir(path: str, include_sol: bool = False, include_instance: bool = True) -> (str, str, str):\n",
    "    dataset_prefix: str = path.split(\"/\")[-1]\n",
    "    for root, _, files in os.walk(path):\n",
    "        dataset: str = dataset_prefix + \"_\" + \"_\".join(root.replace(path, \"\")[1:].split('/'))\n",
    "        for file in files:\n",
    "            if \"column\" not in file and \".dat\" in file:\n",
    "                if (\"_sol\" in file and include_sol) or (\"_inst\" in file and include_instance):\n",
    "                    yield (dataset, root, file)\n",
    "\n",
    "def get_cols_list(path: str):\n",
    "    cols = pandas.read_csv(path, index_col=None, delimiter=\" \", header=None)\n",
    "    return list(cols.iloc[0])\n",
    "\n",
    "def load_sol_from_dir(folder_path: str, column_list, special_column_name: str = \"instance_info\"):\n",
    "    \"\"\" Load solutions from directory files into table. \"\"\"\n",
    "    it = get_file_paths_from_dir(folder_path, include_sol=True, include_instance=False)\n",
    "    output_table = None\n",
    "        \n",
    "    for (dataset, root, file) in it:\n",
    "        curr_table = pandas.read_csv(f'{root}/{file}', index_col=None, delimiter=\" \", header=None).iloc[:,0:3]\n",
    "        curr_table.columns = column_list\n",
    "        curr_table[\"dataset\"] = dataset\n",
    "        curr_table[special_column_name] = file.split(\"_\")[1]\n",
    "                \n",
    "        if output_table is not None:\n",
    "            output_table = output_table.append(curr_table, ignore_index=True)\n",
    "        else:\n",
    "            output_table = curr_table\n",
    "    \n",
    "    return output_table\n",
    "\n",
    "def load_data_from_dir(folder_path: str, column_list, special_column_name: str = \"instance_info\"):\n",
    "    \"\"\" Load data from directory files into table. \"\"\"\n",
    "    it = get_file_paths_from_dir(folder_path)\n",
    "    output_table = None\n",
    "    \n",
    "    for (dataset, root, file) in it:\n",
    "        curr_table = pandas.read_csv(f'{root}/{file}', index_col=None, delimiter=\" \", header=None)\n",
    "        curr_table.columns = column_list\n",
    "        curr_table[\"dataset\"] = dataset\n",
    "        curr_table[special_column_name] = file.split(\"_\")[1]\n",
    "                \n",
    "        if output_table is not None:\n",
    "            output_table = output_table.append(curr_table, ignore_index=True)\n",
    "        else:\n",
    "            output_table = curr_table\n",
    "    \n",
    "    #output_table = output_table.set_index(['algorithm', 'dataset', 'id', \"item_count\"])\n",
    "    #output_table.sort_values(by=[\"algorithm\", \"dataset\", \"item_count\", \"id\"], inplace=True)\n",
    "    return output_table\n",
    "\n",
    "def construct_table_from(filePair: FilePair):\n",
    "    solution_table = pandas.read_csv(filePair.solutionFile, header=None, index_col=None, delimiter=\" \")\n",
    "    data_table = pandas.read_csv(filePair.dataFile, header=None, index_col=None, delimiter=\" \")\n",
    "    \n",
    "    item_count = data_table.iloc[0, 1]\n",
    "    \n",
    "    solution_table = solution_table.drop_duplicates(subset=[0], keep='first').reset_index()\n",
    "\n",
    "    data_table = data_table.iloc[:, 4:]\n",
    "    data_table = data_table[data_table.columns[::2]]\n",
    "\n",
    "    info_table = pandas.concat([solution_table.iloc[:, 1], solution_table.iloc[:, 3], data_table.max(axis=1)], axis=1)\n",
    "    info_table.columns = [\"id\", \"best_value\", \"max_cost\"]\n",
    "    info_table[\"item_count\"] = item_count\n",
    "    return info_table\n",
    "\n",
    "def create_avg_time_table(table, name: str, column_name: str = \"item_count\"):\n",
    "    # Create a table of average times according to algorithm and item_count columns\n",
    "    avg_times = table.groupby([\"algorithm_name\", column_name])['elapsed_time'] \\\n",
    "        .mean().reset_index()\n",
    "    avg_times = avg_times.round(2)\n",
    "    \n",
    "    avg_configs = table.groupby([\"algorithm_name\", column_name])['elapsed_configs']\\\n",
    "        .mean().reset_index()\n",
    "    \n",
    "    avg_times = avg_times.merge(avg_configs, on=[\"algorithm_name\", column_name])\n",
    "\n",
    "    # Move all values of algorithm column into separate columns\n",
    "    #avg_times = avg_times.unstack(\"algorithm_name\")\n",
    "    #avg_times.columns = avg_times.columns.droplevel()\n",
    "    avg_times.name = f\"Avg #configs per {column_name}\"\n",
    "    avg_times.sort_values(by=column_name, inplace=True)\n",
    "    #avg_times.fillna(\"-\", inplace=True)\n",
    "\n",
    "    # Save the dataframe to csv\n",
    "    #avg_times.to_excel(f'excel/{name}_avg_times.xlsx', sheet_name=name)\n",
    "    \n",
    "    return avg_times\n",
    "\n",
    "def create_avg_error_table(table, column_name: str, table_name: str = \"unknown\"):\n",
    "    table[\"relative_error\"] = numpy.abs(table[\"best_value\"] - table[\"found_value\"])/table[\"best_value\"]\n",
    "    table = table.fillna(0)\n",
    "    \n",
    "    error_group = table.groupby([column_name, \"algorithm_name\"])[\"relative_error\"]\n",
    "\n",
    "    error_max = error_group.max().reset_index() \\\n",
    "        .rename(columns={'relative_error':'max_relative_error'})\n",
    "    error_avg = error_group.mean().reset_index() \\\n",
    "        .rename(columns={'relative_error':'avg_relative_error'})\n",
    "    \n",
    "    # Construct, unstack\n",
    "    avg_error = pandas.merge(error_max, error_avg, on=[column_name, \"algorithm_name\"])\n",
    "    #avg_error = avg_error.set_index([\"algorithm_name\", column_name]).unstack(\"algorithm_name\")\n",
    "    #avg_error = error_max.join(error_avg).round(6)\n",
    "    #avg_error.columns = [\"max_relative_error\", \"avg_relative_error\"]\n",
    "    avg_error.name = f\"Avg & max relative error per {column_name}\"\n",
    "    \n",
    "    \n",
    "    avg_error.set_index([\"algorithm_name\", column_name]).unstack(\"algorithm_name\")\\\n",
    "        .round(6)\\\n",
    "        .to_excel(f\"excel/{table_name}_avg_error.xlsx\", sheet_name=table_name)\n",
    "    \n",
    "    return avg_error\n",
    "\n",
    "def save_plot(table, title: str, column_name: str, output_name: str, y_label: str = \"Relative errors\"):\n",
    "    worktable = table.loc[:, column_name].copy()\n",
    "    \n",
    "    plot = worktable.plot()\n",
    "    plot.set_ylabel(y_label)\n",
    "\n",
    "    figure = plot.get_figure()\n",
    "    figure.suptitle(title)\n",
    "    figure.savefig(f\"excel/{output_name}.pdf\")\n",
    "    \n",
    "    return plot\n",
    "\n",
    "def save_table(table, output_name):\n",
    "    table.round(6).to_excel(f\"excel/{output_name}_table.xlsx\", sheet_name=output_name)\n",
    "    \n",
    "def init_evo_plot(title: str):\n",
    "    fig, axes = plt.subplots(1, 1, sharex=True, sharey=True, figsize=[13,5])\n",
    "    axes.set_xlabel('step')\n",
    "    axes.set_ylabel('Sum of costs')\n",
    "    axes.set_title(title)\n",
    "    return fig, axes\n",
    "    \n",
    "def add_evo_plot(path: str, columns, plot_name: str, axes):\n",
    "    evo_table = pandas.read_csv(path, index_col=None, delimiter=\" \", header=None)\n",
    "    evo_table.columns = evo_cols\n",
    "    evo_table[\"step\"] = range(evo_table['current_temperature'].count())\n",
    "    \n",
    "    data = evo_table.sort_values(by=\"step\").set_index([\"step\"]).loc[:, \"best_cost\"]\n",
    "    \n",
    "    axes.plot(data, label=f\"best_cost_{plot_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column lists\n",
    "\n",
    "sol_cols = [\"output_filename\", \"best_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 23 fields in line 267, saw 53\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-10cf4a744bc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mget_all_tables_of_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tester_results_V2_moreAccurate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#pandas.read_csv(\"tester_results_V2_moreAccurate/wuf-M/wuf20-78-M.dat\", index_col=None, delimiter=\" \", header=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-10cf4a744bc3>\u001b[0m in \u001b[0;36mget_all_tables_of_results\u001b[0;34m(base)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mq2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_full_table_for_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{base}/wuf-Q/wuf50-201-Q.dat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{sol_path}/wuf-Q/wuf50-201-Q-opt.dat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msol_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_full_table_for_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{base}/wuf-R/wuf20-78-R.dat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{sol_path}/wuf-R/wuf20-78-R-opt.dat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msol_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_full_table_for_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{base}/wuf-R/wuf50-201-R.dat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{sol_path}/wuf-R/wuf50-201-R-opt.dat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msol_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-10cf4a744bc3>\u001b[0m in \u001b[0;36mget_full_table_for_dataset\u001b[0;34m(instance_path, sol_path, instance_cols, sol_cols)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Add data from solution file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msols_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msol_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msols_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msol_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/LocalShared/AlgorithmTester/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/LocalShared/AlgorithmTester/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/LocalShared/AlgorithmTester/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/LocalShared/AlgorithmTester/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 23 fields in line 267, saw 53\n"
     ]
    }
   ],
   "source": [
    "def get_full_table_for_dataset(instance_path: str, sol_path: str, instance_cols, sol_cols):\n",
    "    output_table = None\n",
    "    \n",
    "    # Get all solutions from all files\n",
    "    output_table = pandas.read_csv(instance_path, index_col=None, delimiter=\" \", header=None)\\\n",
    "        .iloc[:, [0, 1,-9,-8,-7,-6,-5,-4,-3,-2,-1]]\n",
    "    output_table.columns = instance_cols\n",
    "\n",
    "    # Add data from solution file\n",
    "    sols_table = pandas.read_csv(sol_path, index_col=None, delimiter=\" \", header=None)\\\n",
    "        .iloc[:, [0, 1]]\n",
    "    sols_table.columns = sol_cols\n",
    "    sols_table.drop_duplicates(subset=\"output_filename\", inplace=True)\n",
    "        \n",
    "    output_table = pandas.merge(sols_table, output_table, on=[\"output_filename\"])\n",
    "    output_table = output_table.astype({'found_value': 'int64'})\n",
    "    output_table[\"relative_mistake\"] = output_table[\"found_value\"]/output_table[\"best_value\"]\n",
    "    \n",
    "    return output_table\n",
    "\n",
    "def get_all_tables_of_results(base: str):\n",
    "    cols = get_cols_list(f'{base}/column_description.dat')\n",
    "    cols.remove(\"vars_output\")\n",
    "        \n",
    "    #a1 = get_full_table_for_dataset(f'{base}/wuf-A/wuf20-88-A.dat', f'{sol_path}/wuf-A/wuf20-88-A-opt.dat', cols, sol_cols)\n",
    "    a2 = get_full_table_for_dataset(f'{base}/wuf-A/wuf20-91-A.dat', f'{sol_path}/wuf-A/wuf20-91-A-opt.dat', cols, sol_cols)\n",
    "    \n",
    "    #m1 = get_full_table_for_dataset(f'{base}/wuf-M/wuf20-78-M.dat', f'{sol_path}/wuf-M/wuf20-78-M-opt.dat', cols, sol_cols)\n",
    "    m2 = get_full_table_for_dataset(f'{base}/wuf-M/wuf50-201-M.dat', f'{sol_path}/wuf-M/wuf50-201-M-opt.dat', cols, sol_cols)\n",
    "    \n",
    "    #n1 = get_full_table_for_dataset(f'{base}/wuf-N/wuf20-78-N.dat', f'{sol_path}/wuf-N/wuf20-78-N-opt.dat', cols, sol_cols)\n",
    "    n2 = get_full_table_for_dataset(f'{base}/wuf-N/wuf50-201-N.dat', f'{sol_path}/wuf-N/wuf50-201-N-opt.dat', cols, sol_cols)\n",
    "\n",
    "    #q1 = get_full_table_for_dataset(f'{base}/wuf-Q/wuf20-78-Q.dat', f'{sol_path}/wuf-Q/wuf20-78-Q-opt.dat', cols, sol_cols)\n",
    "    q2 = get_full_table_for_dataset(f'{base}/wuf-Q/wuf50-201-Q.dat', f'{sol_path}/wuf-Q/wuf50-201-Q-opt.dat', cols, sol_cols)\n",
    "    \n",
    "    r1 = get_full_table_for_dataset(f'{base}/wuf-R/wuf20-78-R.dat', f'{sol_path}/wuf-R/wuf20-78-R-opt.dat', cols, sol_cols)\n",
    "    r2 = get_full_table_for_dataset(f'{base}/wuf-R/wuf50-201-R.dat', f'{sol_path}/wuf-R/wuf50-201-R-opt.dat', cols, sol_cols)\n",
    "\n",
    "    #dfs = [a1, a2, m1, m2, n1, n2, q1, q2, r1, r2]\n",
    "    dfs = [a1, a2, m2, n2, q2, r1, r2]\n",
    "    \n",
    "    return pandas.concat(dfs)\n",
    "\n",
    "get_all_tables_of_results(\"tester_results_V2_moreAccurate\")\n",
    "\n",
    "#pandas.read_csv(\"tester_results_V2_moreAccurate/wuf-M/wuf20-78-M.dat\", index_col=None, delimiter=\" \", header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put data from all analysis files into tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all tables\n",
    "\n",
    "wuf_A_88 = get_full_table_for_dataset(f'{path}/wuf-A/wuf20-88-A', f'{sol_path}/wuf-A/wuf20-88-A-opt.dat', cols, sol_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_nk = create_avg_error_table(nk_table, \"item_count\")\n",
    "error_zkc = create_avg_error_table(zkc_table, \"item_count\")\n",
    "error_zkw = create_avg_error_table(zkw_table, \"item_count\")\n",
    "\n",
    "time_nk = create_avg_time_table(nk_table, name=\"initial temperature\", column_name=\"item_count\")\n",
    "time_zkc = create_avg_time_table(zkc_table, name=\"cooling coefficient\", column_name=\"item_count\")\n",
    "time_zkw = create_avg_time_table(zkw_table, name=\"number of cycles\", column_name=\"item_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table = nk_table.append(zkc_table).append(zkw_table).query(\"algorithm_name != 'SA_OLD'\")\n",
    "sapenalty_error_comp = create_avg_error_table(full_table, \"item_count\", \"sapenalty_comp\")\\\n",
    "    .set_index([\"item_count\", \"algorithm_name\"]).unstack(\"algorithm_name\")\\\n",
    "    .drop(columns=\"max_relative_error\")\n",
    "\n",
    "sapenalty_comp_plot = sapenalty_error_comp.plot.bar(legend=True)\n",
    "sapenalty_comp_plot.set_ylabel(\"Relative errors\")\n",
    "\n",
    "figure = sapenalty_comp_plot.get_figure()\n",
    "figure.suptitle(\"SA/SAPenalty relative error comparison\")\n",
    "figure.savefig(\"excel/sapenalty_comp.pdf\")\n",
    "\n",
    "sapenalty_comp_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter analysis\n",
    "## Analyze the same dataset, change one of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sols = load_sol_from_dir(f'{solutions}/ParamAnalysis', sol_cols) \\\n",
    "    .drop(columns=\"dataset\")\n",
    "\n",
    "init_temp_table = load_data_from_dir(f'{path}/ParamAnalysis/InitTemperature', cols) \\\n",
    "    .drop(columns=[\"things\", \"cycles\", \"min_temperature\", \"cooling\"]) \\\n",
    "    .merge(param_sols, on=[\"id\", \"item_count\"])\n",
    "init_temp_table[\"init_temp\"] = init_temp_table[\"dataset\"]\\\n",
    "    .str.split(\"_\", n = 1, expand = True)[1].astype(int)\n",
    "\n",
    "cooling_table = load_data_from_dir(f'{path}/ParamAnalysis/Cooling', cols) \\\n",
    "    .drop(columns=[\"things\", \"cycles\", \"min_temperature\", \"init_temperature\"]) \\\n",
    "    .merge(param_sols, on=[\"id\", \"item_count\"])\n",
    "cooling_table[\"cooling_coef\"] = cooling_table[\"dataset\"]\\\n",
    "    .str.split(\"_\", n = 1, expand = True)[1].str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "cycles_table = load_data_from_dir(f'{path}/ParamAnalysis/Cycles', cols) \\\n",
    "    .drop(columns=[\"things\", \"init_temperature\", \"min_temperature\", \"cooling\"]) \\\n",
    "    .merge(param_sols, on=[\"id\", \"item_count\"])\n",
    "cycles_table[\"cycles\"] = cycles_table[\"dataset\"]\\\n",
    "    .str.split(\"_\", n = 1, expand = True)[1].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_init_temp = create_avg_error_table(init_temp_table, \"init_temp\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"init_temp\")\n",
    "error_cooling = create_avg_error_table(cooling_table, \"cooling_coef\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"cooling_coef\")\n",
    "error_cycles = create_avg_error_table(cycles_table, \"cycles\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"cycles\")\n",
    "\n",
    "time_init_temp = create_avg_time_table(init_temp_table, name=\"initial temperature\", column_name=\"init_temp\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"init_temp\")\n",
    "time_cooling = create_avg_time_table(cooling_table, name=\"cooling coefficient\", column_name=\"cooling_coef\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"cooling_coef\")\n",
    "time_cycles = create_avg_time_table(cycles_table, name=\"number of cycles\", column_name=\"cycles\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"cycles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_init_temp)\n",
    "\n",
    "out = pandas.merge(error_init_temp, time_init_temp, on=[\"algorithm_name\", \"init_temp\"])\n",
    "save_table(out, \"init_temp_errors_speed\")\n",
    "save_plot(error_init_temp, \"Avg error - init temperatures\", \"avg_relative_error\", \"init_temp_avg_error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_init_temp)\n",
    "\n",
    "save_plot(time_init_temp, \"Avg speed - init temperatures\", \"elapsed_time\", \"init_temp_time_ms\", \"Time[ms]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_init_temp, \"Avg speed - init temperatures\", \"elapsed_configs\", \"init_temp_time_configs\", \"Time[configs]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = init_evo_plot(\"Evolution of costs for different init temperatures\")\n",
    "\n",
    "add_evo_plot(f\"{evo_path}/InitTemperature/EvoFile_40_inst_SA_1500.evo\", evo_cols, \"1500\", axes)\n",
    "add_evo_plot(f\"{evo_path}/InitTemperature/EvoFile_40_inst_SA_2500.evo\", evo_cols, \"2500\", axes)\n",
    "axes.legend()\n",
    "\n",
    "fig.savefig(f\"excel/init_temp_evo.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_cooling)\n",
    "\n",
    "out = pandas.merge(error_cooling, time_cooling, on=[\"algorithm_name\", \"cooling_coef\"])\n",
    "save_table(out, \"cooling_errors_speed\")\n",
    "save_plot(error_cooling, \"Avg error - cooling\", \"avg_relative_error\", \"cooling_avg_error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_cooling, \"Avg speed - cooling\", \"elapsed_time\", \"cooling_time_ms\", \"Time[ms]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_cooling, \"Avg speed - cooling\", \"elapsed_configs\", \"cooling_time_configs\", \"Time[configs]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = init_evo_plot(\"Evolution of costs for different cooling\")\n",
    "\n",
    "add_evo_plot(f\"{evo_path}/Cooling/EvoFile_40_inst_SA_950.evo\", evo_cols, \"950\", axes)\n",
    "add_evo_plot(f\"{evo_path}/Cooling/EvoFile_40_inst_SA_970.evo\", evo_cols, \"970\", axes)\n",
    "\n",
    "axes.legend()\n",
    "\n",
    "fig.savefig(f\"excel/cooling_evo.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_cycles)\n",
    "\n",
    "out = pandas.merge(error_cycles, time_cycles, on=[\"algorithm_name\", \"cycles\"])\n",
    "save_table(out, \"cycles_errors_speed\")\n",
    "save_plot(error_cycles, \"Avg error - cycles\", \"avg_relative_error\", \"cycles_avg_error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_cycles, \"Avg speed - cycles\", \"elapsed_time\", \"cycles_time_configs\", \"Time[ms]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_cycles, \"Avg speed - cycles\", \"elapsed_configs\", \"cycles_time_ms\", \"Time[configs]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = init_evo_plot(\"Evolution of costs for different cycles\")\n",
    "\n",
    "add_evo_plot(f\"{evo_path}/Cycles/EvoFile_40_inst_SA_75.evo\", evo_cols, \"75\", axes)\n",
    "add_evo_plot(f\"{evo_path}/Cycles/EvoFile_40_inst_SA_100.evo\", evo_cols, \"100\", axes)\n",
    "\n",
    "axes.legend()\n",
    "\n",
    "fig.savefig(f\"excel/cycles_evo.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataAnalysis\n",
    "## Analyze datasets that were generated differently with fixed SA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath = f'{path}/DataAnalysis'\n",
    "dsolutions = f'{solutions}/DataAnalysis'\n",
    "\n",
    "balance_table = load_data_from_dir(f'{dpath}/Balance', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/Balance', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'balance'})\n",
    "correlation_table = load_data_from_dir(f'{dpath}/Correlation', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/Correlation', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'correlation'})\n",
    "granularity_heavy_table = load_data_from_dir(f'{dpath}/GranularityHeavy', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/GranularityHeavy', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'constant'})\n",
    "granularity_light_table = load_data_from_dir(f'{dpath}/GranularityLight', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/GranularityLight', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'constant'})\n",
    "maxcost_table = load_data_from_dir(f'{dpath}/MaxCost', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/MaxCost', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'maxcost'})\n",
    "maxweight_table = load_data_from_dir(f'{dpath}/MaxWeight', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/MaxWeight', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'maxweight'})\n",
    "things_table = load_data_from_dir(f'{dpath}/Things', cols)\n",
    "weight_cap_ratio_table = load_data_from_dir(f'{dpath}/WeightCapRation', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/WeightCapRation', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'ratio'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_balance = create_avg_error_table(balance_table, \"balance\")\n",
    "error_corr = create_avg_error_table(correlation_table, \"correlation\")\n",
    "error_granularity_heavy = create_avg_error_table(granularity_heavy_table, \"constant\")\n",
    "error_granularity_light = create_avg_error_table(granularity_light_table, \"constant\")\n",
    "error_maxcost = create_avg_error_table(maxcost_table, \"maxcost\")\n",
    "error_maxweight = create_avg_error_table(maxweight_table, \"maxweight\")\n",
    "error_weightcap = create_avg_error_table(weight_cap_ratio_table, \"ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_avg_error_table(, \"maxcost\")\n",
    "print(error_balance.query('algorithm_name == \"SA\"'))\n",
    "error_balance.query('algorithm_name == \"SA\"').set_index(\"balance\").loc[:, \"avg_relative_error\"].plot.bar(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_corr.query('algorithm_name == \"SA\"'))\n",
    "error_corr.query('algorithm_name == \"SA\"').set_index(\"correlation\").loc[:, \"avg_relative_error\"].plot.bar(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_granularity_heavy.query('algorithm_name == \"SA\"'))\n",
    "error_granularity_heavy.query('algorithm_name == \"SA\"').set_index(\"constant\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_granularity_light.query('algorithm_name == \"SA\"'))\n",
    "error_granularity_light.query('algorithm_name == \"SA\"').set_index(\"constant\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_maxcost.query('algorithm_name == \"SA\"'))\n",
    "error_maxcost.query('algorithm_name == \"SA\"').set_index(\"maxcost\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_maxweight.query('algorithm_name == \"SA\"'))\n",
    "error_maxweight.query('algorithm_name == \"SA\"').set_index(\"maxweight\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_weightcap.query('algorithm_name == \"SA\"'))\n",
    "error_weightcap.query('algorithm_name == \"SA\"').set_index(\"ratio\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
